---
title: IBM Event Streams Schema Registry
toc: false
sidebar: labs_sidebar
folder: pots/msghub
permalink: /msghub_pot_lab14.html
summary: Schema Registry
applies_to: [developer,administrator]
---

# Using the schema registry in IBM Event Streams

## Overview

IBM Event Streams can handle any data, but it does not validate the information in the messages. However, efficient handling of data often requires that it includes specific information in a certain format. Using schemas, you can define the structure of the data in a message, ensuring that both producers and consumers use the correct structure.

Schemas help producers create data that conforms to a predefined structure, defining the fields that need to be present together with the type of each field. This definition then helps consumers parse that data and interpret it correctly. Event Streams supports schemas and includes a schema registry for using and managing schemas.

It is common for all of the messages on a topic to use the same schema. The key and value of a message can each be described by a schema. 

![](./images/pots/msghub/lab14/image1.png)

## Introduction

Schemas are stored in the Event Streams schema registry. In addition to storing a versioned history of schemas, it provides an interface for retrieving them. Each Event Streams cluster has its own schema registry.

Your producers and consumers validate the data against the specified schema stored in the schema registry. This is in addition to going through Kafka brokers. The schemas do not need to be transferred in the messages this way, meaning the messages are smaller than without using a schema registry.

![](./images/pots/msghub/lab14/image2.png)

If you are migrating to use Event Streams as your Kafka solution, and have been using a schema registry from a different provider, you can migrate to using the Event Streams schema registry.

### Apache Avro data format

Schemas are defined using Apache Avro, an open-source data serialization technology commonly used with Apache Kafka. It provides an efficient data encoding format, either by using the compact binary format or a more verbose, but human-readable JSON format.

The Event Streams schema registry uses Apache Avro data formats. When messages are sent in the Avro format, they contain the data and the unique identifier for the schema used. The identifier specifies which schema in the registry is to be used for the message.

Avro has support for a wide range of data types, including primitive types (null, boolean, int, long, float, double, bytes, and string) and complex types (record, enum, array, map, union, and fixed).

![](./images/pots/msghub/lab14/image3.png)

### Serialization and deserialization

A producing application uses a serializer to produce messages conforming to a specific schema. As mentioned earlier, the message contains the data in Avro format, together with the the schema identifier.

A consuming application then uses a deserializer to consume messages that have been serialized using the same schema. When a consumer reads a message sent in Avro format, the deserializer finds the identifier of the schema in the message, and retrieves the schema from the schema registry to deserialize the data.

This process provides an efficient way of ensuring that data in messages conform to the required structure.

Serializers and deserializers that automatically retrieve the schemas from the schema registry as required are provided or generated by IBM Event Streams. If you need to use schemas in an environment for which serializers or deserializers are not provided, you can use the command line or UI directly to retrieve the schemas.

![](./images/pots/msghub/lab14/image4.png)

### Versions and compatibility

Whenever you add a schema, and any subsequent versions of the same schema, Event Streams validates the format automatically and warns of any issues. You can evolve your schemas over time to accommodate changing requirements. You simply create a new version of an existing schema, and the schema registry ensures that the new version is compatible with the existing version, meaning that producers and consumers using the existing version are not broken by the new version.

When you create a new version of the schema, you simply add it to the registry and version it. You can then set your producers and consumers that use the schema to start using the new version. Until they do, both producers and consumers are warned that a new version of the schema is available.

![](./images/pots/msghub/lab14/image5.png)

### Lifecycle

When a new version is used, you can deprecate the previous version. Deprecating means that producing and consuming applications still using the deprecated version are warned that a new version is available to upgrade to. When you upgrade your producers to use the new version, you can disable the older version so it can no longer be used, or you can remove it entirely from the schema registry.

You can use the Event Streams UI or CLI to manage the lifecycle of schemas, including registering, versioning, deprecating, and so on.

![](./images/pots/msghub/lab14/image6.png)

## Creating and adding schemas

You can create schemas in Avro format. You can then use the Event Streams UI or CLI to add the schemas to the schema registry.

### Creating schemas

Event Streams supports Apache Avro schemas. Avro schemas are written in JSON to define the format of the messages. For more information about Avro schemas, see the Avro documentation. 

The Event Streams schema registry imports, stores, and uses Avro schemas to serialize and deserialize Kafka messages. The schema registry supports Avro schemas using the record complex type. The record type can include multiple fields of any data type, primitive or complex.

Define your Avro schema files and save them by using the .avsc or .json file extension.

1. Open a terminal window and enter:

	```
	gedit book.json
	```
	
	Copy and the code below and paste into the edit window.
	
	```
	{
	  "type": "record",
	  "name": "Book",
	  "namespace": "org.example",
	  "fields": [
	    {
	      "name": "Title",
	      "type": "string"
	    },
	    {
	      "name": "Author",
	      "type": "string"
	    },
	    {
	      "name": "Format",
	      "type": {
	        "type": "enum",
	        "name": "Booktype",
	        "symbols": [
	          "HARDBACK",
	          "PAPERBACK"
	        ]
	      }
	    }
	  ]
	}
	```
	
1. Click *Save*. 

	![](./images/pots/msghub/lab14/image9.png)	
1. While still in the editor click *Open*. Enter a new file name **book-new.json** and hit enter. You will use this file to revise your schema later.

	![](./images/pots/msghub/lab14/image14.png)
	
1. Copy this code and paste into the new editor window.

```
	{
	  "type": "record",
	  "name": "Book",
	  "namespace": "org.example",
	  "fields": [
	    {
	      "name": "Title",
	      "type": "string"
	    },
	    {
	      "name": "Author",
	      "type": "string"
	    },
	    {
	      "name": "Format",
	      "type": {
	        "type": "enum",
	        "name": "Booktype",
	        "symbols": [
	          "HARDBACK",
	          "PAPERBACK"
	        ]
	      }
	    },
	    {
	      "name": "PageCount",
	      "type": "int",
	      "default": 0
	    }
	  ]
	}
```

1. Click *Save* and close the editor.  *Open*. Enter a new file name **book-new.json** and hit enter. 

	![](./images/pots/msghub/lab14/image145png)


### Adding schemas to the registry

To use schemas in Kafka applications, import your schema definitions into the schema registry. Your applications can then retrieve the schemas from the registry as required.

1. Log in to your Event Streams UI as an administrator from a supported web browser.

1. Click Schema Registry in the primary navigation, and then click *Add schema*.

	![](./images/pots/msghub/lab14/image7.png)

1. Click *Upload definition* and select your Avro schema file **book.json**. (Avro schema files use the .avsc or .json file extensions.)

	Click *Open*.

	![](./images/pots/msghub/lab14/image10.png)

 The file is loaded and its format validated. If the validation finds any problems with the file, a warning message is displayed. 
 
 	![](./images/pots/msghub/lab14/image11.png)

1. Click *Add schema*. You are notified that the schema is being added. After a few seconds the schema is added to the list of schemas in the Event Streams schema registry.

	![](./images/pots/msghub/lab14/image12.png)
		 
1. Optional: Edit the Schema name and Version fields.
The name of the record defined in the Avro schema file is added to the Schema name field. You can edit this field to add a different name for the schema. Changing the Schema name field does not update the Avro schema definition itself.
The value 1.0.0 is automatically added to the Version field as the initial version of the schema. You can edit this field to set a different version number for the schema.

### Adding new schema versions

The Event Streams schema registry can store multiple versions of the same schema. As your applications and environments evolve, your schemas need to change to accommodate the requirements. You can import, manage, and use different versions of a schema. As your schemas change, consider the options for managing their lifecycle. 

	 {% include note.html content="A new version of a schema must be compatible with previous versions. This means that messages that have been serialized with an earlier version of a schema can be deserialized with a later version. To be compatible, fields in later versions of a schema cannot be removed, and any new schema field must have a default value." %}

For example, the following Avro schema defines a new version of the Book record, adding a PageCount field. By including a default value for this field, messages that were serialized with the previous version of this schema (which would not have a PageCount value) can still be deserialized using this version.

1. Locate your schema in the list of registered schemas and click its name. The list of versions for the schema is displayed.

1. Click *Add new version* to add a new version of the schema.

	![](./images/pots/msghub/lab14/image13.png)

1. Click Upload definition and select the file that contains the new version of your schema. 

	Click *Open*. The file is loaded and its format validated. If the validation finds any problems with the file, a warning message is displayed.

	![](./images/pots/msghub/lab14/image16.png)
	
1. Click *Add \+* to set a value in the Version field to be the version number for this iteration of the schema. 

	![](./images/pots/msghub/lab14/image17.png)
	
1. Enter **1.0.1** in the *Version* field, then click *Add schema*. The schema version is added to the list of all versions for the schema.

	![](./images/pots/msghub/lab14/image18.png)
	
	Now you see both versions in the **Book** schema. 	
1.	For the current list of all versions, click View all versions.
	
	![](./images/pots/msghub/lab14/image20.png)

### Managing schema lifecycle

You can have multiple versions of a schema stored in the Event Streams schema registry. Kafka producers and consumers retrieve the right schema version they use from the registry based on a unique identifier and version.

When a new schema version is added, you can set both the producer and consumer applications to use that version. You then have the following options to handle earlier versions. The lifecycle is as follows:


* Add schema
* Add new schema version
* Deprecate earlier versions or deprecate entire schema
* Disable version or entire schema
* Remove version or entire schema

### Deprecating

If you want your applications to use a new version of a schema, you can set the earlier version to Deprecated. When a version is deprecated, the applications using that version receive a message to warn them to stop using it. Applications can continue to use the schema, but warnings will be written to application logs about the schema version being deprecated. You can customize the message to be provided in the logs.

Deprecated versions are still available in the registry and can be used again.

 {% include note.html content="You can deprecate a entire schema, not just the versions of that schema. If the entire schema is set to deprecated, then all of its versions are reported as deprecated (including any new ones added)." %}

1. Click the *Schema registry* breadcrumb to return to the list of schemas.

	![](./images/pots/msghub/lab14/image21.png)

1. Select the book schema you want to deprecate from the list. 

1. Set the entire schema or a selected version of the schema to be deprecated

	* If you want to deprecate the entire schema and all its versions, click the Manage schema tab, and set Mark schema as deprecated to on.
	* To deprecate a specific version, select it from the list, and click the Manage version tab for that version. Then set Mark schema as deprecated to on.

	Click *Manage schema*. 
	
	![](./images/pots/msghub/lab14/image22.png)

1. Click the *Mark schema as deprecated* switch to **ON**.
	
	![](./images/pots/msghub/lab14/image23.png)

1. Deprecated schemas and versions are marked with a Deprecated flag on the UI. Click the *Schema Registry* breadcrumb to return to schema list. Click the *book* schema again. It now shows deprecated. Since you deprecated the full schema, both versions show deprecated.

	![](./images/pots/msghub/lab14/image27.png)
	
1. You can re-activate a schema or its version by setting Mark schema as deprecated to off. Click *Manage schema* and turn off the deprecation switch.

	![](./images/pots/msghub/lab14/image28.png)

### Disabling

If you want your applications to stop using a specific schema, you can set the schema version to Disabled. If you disable a version, applications will be prevented from producing and consuming messages using it. You can re-enable it again to allow applications to use the schema again.

When a schema is disabled, applications that want to use the schema receive an ERROR.

Java producers using the Event Streams schema registry serdes library will throw a SchemaDisabledException when attempting to produce messages using a disabled schema version.

For example, the message and stack trace for a disabled schema named Test_Schema would look like this:

```
com.ibm.eventstreams.serdes.exceptions.SchemaDisabledException: Schema "Test_Schema" version "1.0.0" is disabled.
	at com.ibm.eventstreams.serdes.EventStreamsSerializer.serialize(EventStreamsSerializer.java:174)
	at com.ibm.eventstreams.serdes.EventStreamsSerializer.serialize(EventStreamsSerializer.java:41)
	at org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:884)
	at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:846)
	at org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:733)
	at Producer.main(Producer.java:92)
```

 {% include note.html content="You can disable a entire schema, not just the versions of that schema. If the entire schema is disabled, then all of its versions are disabled as well, which means no version of the schema can be used by applications (including any new ones added)." %}

1. Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).

1. Click Schema Registry in the primary navigation.

1. Select the schema you want to disable from the list.

1. Set the entire schema or a selected version of the schema to be disabled:
	
	* If you want to disable the entire schema and all its versions, click the Manage schema tab, and click Disable schema, then click Disable.
	* To disable a specific version, select it from the list, and click the Manage version tab for that version. Then click Disable version, then click Disable.

	You can re-enable a schema by clicking Enable schema, and re-enable a schema version by clicking Re-enable version.
	
### Removing

If a schema version has not been used for a period of time, you can remove it from the schema registry. Removing a schema version means it will be permanently deleted from the schema registry of your Event Streams instance, and applications will be prevented from producing and consuming messages using it.

If a schema is no longer available in the registry, Java applications that want to use the schema receive a SchemaNotFoundException message.

For example, the message and stack trace when producing a message with a missing schema named Test_Schema would look like this:

```
com.ibm.eventstreams.serdes.exceptions.SchemaNotFoundException: Schema "Test_Schema" not found
    at com.ibm.eventstreams.serdes.SchemaRegistryRestAPIClient.handleErrorResponse(SchemaRegistryRestAPIClient.java:145)
    at com.ibm.eventstreams.serdes.SchemaRegistryRestAPIClient.get(SchemaRegistryRestAPIClient.java:120)
    at com.ibm.eventstreams.serdes.SchemaRegistry.downloadSchema(SchemaRegistry.java:253)
    at com.ibm.eventstreams.serdes.SchemaRegistry.getSchema(SchemaRegistry.java:239)
```

 {% include important.html content="You cannot reverse the removal of a schema. This action is permanent." %}

 {% include note.html content="You can remove a entire schema, including all of its versions. If the entire schema is removed, then all of its versions are permanently deleted from the schema registry of your Event Streams instance." %}

1. Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).

1. Click Schema Registry in the primary navigation.

1. Select the schema you want to remove from the list.

1. Remove the entire schema or a selected version of the schema:

	* If you want to remove the entire schema and all its versions, click the Manage schema tab, and click Remove schema, then click Remove.

	* To remove a specific version, select it from the list, and click the Manage version tab for that version. Then click Remove version, then click Remove.

 {% include important.html content="This action is permanent and cannot be reversed." %} 

### Setting Java applications to use schemas

If you have Kafka producer or consumer applications written in Java, use the following guidance to set them up to use schemas.

 {% include note.html content="If you have Kafka clients written in other languages than Java, see the guidance about setting up non-Java applications to use schemas." %}

#### Preparing the setup

To use schemas stored in the Event Streams schema registry, your client applications need to be able to serialize and deserialize messages based on schemas.

* Producing applications use a serializer to produce messages conforming to a specific schema, and use unique identifiers in the message headers to determine which schema is being used.

* Consuming application then use a deserializer to consume messages that have been serialized using the same schema. The schema is retrieved from the schema registry based on the unique identifiers in the message headers.

The Event Streams UI provides help with setting up your Java applications to use schemas.

To set up your Java applications to use the Event Streams schemas and schema registry, prepare the connection for your application as follows:

1. Select a schema from the list and click the row for the schema.

1. Click Connect to the latest version. Alternatively, if you want to use a different version of the schema, click the row for the schema version, and click Connect to this version.

	![](./images/pots/msghub/lab14/image24.png)

1. Set the preferences for your connection in the *Configure the schema connection* section.

	![](./images/pots/msghub/lab14/image24.png)

1. Click *Change configuration* and review the options, but do not change anything.

	* For producers, set the method for Message encoding.

		* Binary (default): Binary-encoded messages are smaller and typically quicker to process. However the message data is not human-readable without an application that is able to apply the schema.
		
		* JSON: JSON-encoded messages are human-readable and can still be used by consumers that are not using the IBM Event Streams schema registry.

	* For consumers, set the Message deserialization behavior for the behavior to use when an application encounters messages that do not conform to the schema.

		* Strict (default): Strict behavior means the message deserializer will fail to process non-conforming messages, throwing an exception if one is encountered.
		
		* Permissive: Permissive behavior means the message deserializer will return a null message when a non-conforming message is encountered. It will not throw an exception, and so allow a Kafka consumer to continue to process further messages.
	* For both producers and consumers, set in the Use generated or generic code section whether your schema is to use custom Java classes that are generated based on the schema, or generic code by using the Apache Avro API.

		* Use schema-specific code (default): Your application will use custom Java classes that are generated based on this schema, using get and set methods to create and access objects. When you want to use a different schema, you will need to update your code to use a new set of specific schema Java classes.
		* Use generic Apache Avro schema code: Your application will create and access objects using the generic Apache Avro API. Producers and consumers that use the generic serializer and deserializer can be coded to produce or consume messages using any schema uploaded to this schema registry.

1. We will use the default values so click *Close*.

1. Go to the Provide an API key section and click *Generate API key*.and follow the instructions.
The API key grants your application access to the cluster and its resources.

1. Click Generate connection details.

1. Download the Java truststore file which contains the server certificate.

1. Click Java dependencies to download the Event Streams schema registry JAR files, and click Schema JAR to download the schema JAR file to use for your application in its code.
 Alternatively, if you are using Maven, click the Use Maven tab. Follow the instructions to copy the configuration snippets for the Event Streams Maven repository to your project Maven POM file, and run the Maven install command to download and install project dependencies.

1. Depending on your application, click the Producer or Consumer tab, and copy the sample Java code snippets displayed. The sample code snippets include the settings you configured to set up your applications to use the schema.

1. Add the snippets into your application code as described in the following sections.

#### Setting up producers to use schemas

1. Ensure you have prepared for the setup, including configuring connection settings, downloading Java dependencies and schema JAR files, and copying code snippets for a producing application.
1. Ensure you add the location of the JAR files to the build path of your producer Kafka application.
1. Use the code snippets from the UI and add them to your application code.

The code snippet from the Imports section includes Java imports to paste into your class, and sets the up the application to use the Event Streams schema registry serdes library and any generated schema-specific classes, for example:

```
import java.util.Properties;

// Import the specific schema class
import com.mycompany.schemas.ABC_Assets_Schema;

import org.apache.kafka.clients.CommonClientConfigs;
import org.apache.kafka.common.config.SaslConfigs;
import org.apache.kafka.common.config.SslConfigs;
import com.ibm.eventstreams.serdes.SchemaRegistryConfig;
import com.ibm.eventstreams.serdes.SchemaInfo;
import com.ibm.eventstreams.serdes.SchemaRegistry;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
```

The code snippet from the Connection properties section specifies connection and access permission details to your Event Streams cluster, for example:

```
Properties props = new Properties();
props.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, "192.0.2.171:30342");
props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_SSL");
props.put(SslConfigs.SSL_PROTOCOL_CONFIG, "TLSv1.2");
props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, <Java_truststore_file_location>);
props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "password");
props.put(SaslConfigs.SASL_MECHANISM, "PLAIN");
String saslJaasConfig = "org.apache.kafka.common.security.plain.PlainLoginModule required "
    + "username=\"token\" password=\"<api_key>";";
props.put(SaslConfigs.SASL_JAAS_CONFIG, saslJaasConfig);

props.put(SchemaRegistryConfig.PROPERTY_API_URL, "https://192.0.2.171:30546");
props.put(SchemaRegistryConfig.PROPERTY_API_SKIP_SSL_VALIDATION, true);
```

{% include note.html content="Replace the <Java_truststore_file_location> with the path to the Java truststore file you downloaded earlier and replace <api_key> with an API key which has the permissions needed for your application." %}

The values are filled in as part of the process of preparing for the setup, setting the correct Kafka configuration properties, including settings such as the API endpoint of your Event Streams installation PROPERTY_API_URL.

For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference.

The code snippet from the Producer code section defines properties for the producer application that set it to use the schema registry and the correct schema, for example:

```
// Set the value serializer for produced messages to use the Event Streams serializer
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "com.ibm.eventstreams.serdes.EventStreamsSerializer");

// Set the encoding type used by the message serializer
props.put(SchemaRegistryConfig.PROPERTY_ENCODING_TYPE, SchemaRegistryConfig.ENCODING_BINARY);

// Get a new connection to the Schema Registry
SchemaRegistry schemaRegistry = new SchemaRegistry(props);

// Get the schema from the registry
SchemaInfo schema = schemaRegistry.getSchema("ABC_Assets_Schema", "1.0.0");

// Get a new specific KafkaProducer
KafkaProducer<String, ABC_Assets_Schema> producer = new KafkaProducer<>(props);

// Get a new specific record based on the schema
ABC_Assets_Schema specificRecord = new ABC_Assets_Schema();

// Add fields and values to the specific record, for example:
// specificRecord.setTitle("this is the value for a title field");

// Prepare the record, adding the Schema Registry headers
ProducerRecord<String, ABC_Assets_Schema> producerRecord =
    new ProducerRecord<String, ABC_Assets_Schema>(<my_topic>, specificRecord);

producerRecord.headers().add(SchemaRegistryConfig.HEADER_SCHEMA_ID,
    schema.getIdAsBytes());
producerRecord.headers().add(SchemaRegistryConfig.HEADER_SCHEMA_VERSION,
    schema.getVersionAsBytes());

// Send the record to Kafka
producer.send(producerRecord);

// Close the producer
producer.close();
```

The Kafka configuration property value.serializer is set to com.ibm.eventstreams.serdes.EventStreamsSerializer, telling Kafka to use the Event Streams serializer for message values when producing messages. You can also use the Event Streams serializer for message keys.

Other values are filled in based on the selected configuration, setting the correct Kafka configuration properties, including settings such as the message encoding behavior SchemaRegistryConfig.PROPERTY_ENCODING_TYPE.

For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference.

 {% include note.html content="Use the generic or generated schema-specific Java classes to set the field values in your message." %}

* Specific Java classes that are generated from the schema definition will have set<field-name> methods that can be used to easily set the field values. For example, if the schema has a field named Author with type string, the generated schema-specific Java class will have a method named setAuthor which takes a string argument value.

* The Generic configuration option will use the org.apache.avro.generic.GenericRecord class. Use the put method in the GenericRecord class to set field names and values.

 {% include note.html content="Replace <my_topic> with the name of the topic to produce messages to." %}

#### Setting up consumers to use schemas

1. Ensure you have prepared for the setup, including configuring connection settings, downloading Java dependencies and schema JAR files, and copying code snippets for a consuming application.

1. Ensure you add the location of the JAR files to the build path of your consumer Kafka application.

1. Use the code snippets from the UI and add them to your application code.

The code snippet from the Imports section includes Java imports to paste into your class, and sets the up the application to use the Event Streams schema registry serdes library and any generated schema-specific classes, for example:

```
import java.time.Duration;
import java.util.Arrays;
import java.util.Properties;

// Import the specific schema class
import com.mycompany.schemas.ABC_Assets_Schema;

import org.apache.kafka.clients.CommonClientConfigs;
import org.apache.kafka.common.config.SaslConfigs;
import org.apache.kafka.common.config.SslConfigs;
import com.ibm.eventstreams.serdes.SchemaRegistryConfig;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.KafkaConsumer;
```

The code snippet from the Connection properties section specifies connection and access permission details to your Event Streams cluster, for example:

```
Properties props = new Properties();
props.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, "192.0.2.171:30342");
props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_SSL");
props.put(SslConfigs.SSL_PROTOCOL_CONFIG, "TLSv1.2");
props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, <Java_truststore_file_location>);
props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "password");
props.put(SaslConfigs.SASL_MECHANISM, "PLAIN");
String saslJaasConfig = "org.apache.kafka.common.security.plain.PlainLoginModule required "
    + "username=\"token\" password=\"<api_key>";";
props.put(SaslConfigs.SASL_JAAS_CONFIG, saslJaasConfig);

props.put(SchemaRegistryConfig.PROPERTY_API_URL, "https://192.0.2.171:30546");
props.put(SchemaRegistryConfig.PROPERTY_API_SKIP_SSL_VALIDATION, true);
```

{% include note.html content="Replace the <Java_truststore_file_location> with the path to the Java truststore file you downloaded earlier and replace <api_key> with an API key which has the permissions needed for your application." %}

The values are filled in as part of the process of preparing for the setup, setting the correct Kafka configuration properties, including settings such as the API endpoint of your Event Streams installation PROPERTY_API_URL.

For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference.

The code snippet from the Consumer code section defines properties for the consumer application that set it to use the schema registry and the correct schema, for example:

```
// Set the value deserializer for consumed messages to use the Event Streams deserializer
props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
props.put("value.deserializer", "com.ibm.eventstreams.serdes.EventStreamsDeserializer");

// Set the behavior of the deserializer when a record cannot be deserialized
props.put(SchemaRegistryConfig.PROPERTY_BEHAVIOR_TYPE, SchemaRegistryConfig.BEHAVIOR_STRICT);

// Set the consumer group ID in the properties
props.put("group.id", <my_consumer_group>);

// Get a new KafkaConsumer
KafkaConsumer<String, ABC_Assets_Schema> consumer = new KafkaConsumer<>(props);

// Subscribe to the topic
consumer.subscribe(Arrays.asList(<my_topic>));

// Poll the topic to retrieve records
while(true) {

    ConsumerRecords<String, ABC_Assets_Schema> records = consumer.poll(Duration.ofSeconds(5));

    for (ConsumerRecord<String, ABC_Assets_Schema> record : records) {
        ABC_Assets_Schema specificRecord = record.value();

        // Get fields and values from the specific record, for example:
        // String titleValue = specificRecord.getTitle().toString();
    }
}
```

The Kafka configuration property value.serializer is set to com.ibm.eventstreams.serdes.EventStreamsDeserializer, telling Kafka to use the Event Streams deserializer for message values when consuming messages. You can also use the Event Streams deserializer for message keys.

Other values are filled in based on the selected configuration, setting the correct Kafka configuration properties, including settings such as the message deserialization behavior SchemaRegistryConfig.PROPERTY_BEHAVIOR_TYPE.

For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference.

 {% include note.html content="Use the generic or generated schema-specific Java classes to read the field values from your message." %}

* Specific Java classes that are generated from the schema definition will have get<field-name> methods that can be used to easily retrieve the field values. For example, if the schema has a field named Author with type string, the generated schema-specific Java class will have a method named getAuthor which returns a string argument value.
* The Generic configuration option will use the org.apache.avro.generic.GenericRecord class. Use the get method in the GenericRecord class to set field names and values.

{% include note.html content="Replace <my_consumer_group> with the name of the consumer group to use and <my_topic> with the name of the topic to consume messages from." %}

#### Setting up Kafka Streams applications

Kafka Streams applications can also use the Event Streams schema registry serdes library to serialize and deserialize messages. For example:

```
// Set the Event Streams serdes properties, including the override option to set the schema
// and version used for serializing produced messages.
Map<String, Object> serdesProps = new HashMap<String, Object>();
serdesProps.put(SchemaRegistryConfig.PROPERTY_API_URL, "https://192.0.2.171:30546");
serdesProps.put(SchemaRegistryConfig.PROPERTY_API_SKIP_SSL_VALIDATION, true);
serdesProps.put(SchemaRegistryConfig.PROPERTY_SCHEMA_ID_OVERRIDE, "ABC_Assets_Schema");
serdesProps.put(SchemaRegistryConfig.PROPERTY_SCHEMA_VERSION_OVERRIDE, "1.0.0");
serdesProps.put(SslConfigs.SSL_PROTOCOL_CONFIG, "TLSv1.2");
serdesProps.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, <Java_truststore_file_location>);
serdesProps.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "password");
serdesProps.put(SaslConfigs.SASL_JAAS_CONFIG, "org.apache.kafka.common.security.plain.PlainLoginModule required "
    + "username=\"token\" password=\"<api_key>\";");

// Set up the Kafka StreamsBuilder
StreamsBuilder builder = new StreamsBuilder();

// Configure a Kafka Serde instance to use the Event Streams schema registry
// serializer and deserializer for message values
Serde<IndexedRecord> valueSerde = new EventStreamsSerdes();
valueSerde.configure(serdesProps, false);

// Get the stream of messages from the source topic, deserializing each message value with the
// Event Streams deserializer, using the schema and version specified in the message headers.
builder.stream(<my_source_topic>, Consumed.with(Serdes.String(), valueSerde))
    // Get the 'nextcount' int field from the record.
    // The Event Streams deserializer constructs instances of the generated schema-specific
    // ABC_Assets_Schema_Count class based on the values in the message headers.
    .mapValues(new ValueMapper<IndexedRecord, Integer>() {
        @Override
        public Integer apply(IndexedRecord val) {
            return ((ABC_Assets_Schema_Count) val).getNextcount();
        }
    })
    // Get all the records
    .selectKey((k, v) -> 0).groupByKey()
    // Sum the values
    .reduce(new Reducer<Integer>() {
        @Override
        public Integer apply(Integer arg0, Integer arg1) {
            return arg0 + arg1;
        }
    })
    .toStream()
    // Map the summed value to a field in the schema-specific generated ABC_Assets_Schema class
    .mapValues(
        new ValueMapper<Integer, IndexedRecord>() {
            @Override
            public IndexedRecord apply(Integer val) {
                ABC_Assets_Schema record = new ABC_Assets_Schema();
                record.setSum(val);
                return record;
            }
     })
     // Finally, put the result to the destination topic, serializing the message value
     // with the Event Streams serializer, using the overridden schema and version from the
     // configuration.
    .to(<my_destination_topic>, Produced.with(Serdes.Integer(), valueSerde));

// Create and start the stream
final KafkaStreams streams = new KafkaStreams(builder.build(), streamsConfig);
streams.start();
```

In this example, the Kafka StreamsBuilder is configured to use the com.ibm.eventstreams.serdes.EventStreamsSerdes class, telling Kafka to use the Event Streams deserializer for message values when consuming messages and the Event Streams serializer for message values when producing messages.

 {% include note.html content="The Kafka Streams org.apache.kafka.streams.kstream API does not provide access to message headers, so to produce messages with the Event Streams schema registry headers, use the SchemaRegistryConfig.PROPERTY_SCHEMA_ID_OVERRIDE and SchemaRegistryConfig.PROPERTY_SCHEMA_VERSION_OVERRIDE configuration properties. Setting these configuration properties will mean produced messages are serialized using the provided schema version and the Event Streams schema registry message headers will be set." %}

For more information about the Event Streams schema registry configuration keys and values, see the SchemaRegistryConfig class in the schema API reference.

 {% include note.html content="TTo re-use this example, replace the <Java_truststore_file_location> with the path to the Java truststore file you downloaded earlier, <api_key> with an API key which has read permissions for your Event Streams deployment, <my_source_topic> with the name of the topic to consume messages from and <my_destination_topic> with the name of the topic to produce messages to." %}

### Setting non-Java applications to use schemas

If you have producer or consumer applications created in languages other than Java, use the following guidance to set them up to use schemas. You can also use the REST producer API to send messages that are encoded with a schema.

### For a producer application:

1. Retrieve the schema definition that you will be using from the Event Streams schema registry and save it in a local file.

1. Use an Apache Avro library for your programming language to read the schema definition from the local file and encode a Kafka message with it.

1. Set the schema registry headers in the Kafka message, so that consumer applications can understand which schema and version was used to encode the message, and which encoding format was used.

1. Send the message to Kafka.

### For a consumer application:

1. Retrieve the schema definition that you will be using from the Event Streams schema registry and save it in a local file.

1. Consume a message from Kafka.

1. Check the headers for the Kafka message to ensure they match the expected schema ID and schema version ID.

1. Use the Apache Avro library for your programming language to read the schema definition from the local file and decode the Kafka message with it.

### Retrieving the schema definition from the schema registry

1. Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).

1. Click Schema Registry in the primary navigation and find your schema in the list.

1. Copy the schema definition into a new local file.
	
	* For the latest version of the schema, expand the row. Copy and paste the schema definition into a new local file.
	
	* For a different version of the schema, click on the row and then select the version to use from the list of schema versions. Click the Schema definition tab and then copy and paste the schema definition into a new local file.

### Setting headers in the messages you send to Event Streams Kafka

Set the following headers in the message to enable applications that use the Event Streams serdes Java library to consume and deserialize the messages automatically. Setting these headers also enables the Event Streams UI to display additional details about the message.

The required message header keys and values are listed in the following table.

| Header Name | Header Key | Header Value |
|:----------:|:-------:|:----------:|
| Schema ID | com.ibm.eventstreams.schemaregistry.schema.id |  The schema ID as a string. | 
| Schema version ID | com.ibm.eventstreams.schemaregistry.schema.version |  The schema version ID as a string. | 
| Message encoding | com.ibm.eventstreams.schemaregistry.encoding | Either JSON for Avro JSON encoding, or BINARY for Avro binary encoding. |


{% include note.html content="The schema version ID is the integer ID that is displayed when listing schema versions using the command cloudctl es schema <schema-name>." %}

### Migrating existing applications to the Event Streams schema registry

If you are using the Confluent Platform schema registry, Event Streams provides a migration path for moving your Kafka consumers and producers over to use the Event Streams schema registry.

#### Migrating schemas to Event Streams schema registry

To migrate schemas, you can use schema auto-registration in your Kafka producer, or you can manually migrate schemas by downloading the schema definitions from the Confluent Platform schema registry and adding them to the Event Streams schema registry. 

##### Migrating schemas with auto-registration

When using auto-registration, the schema will be automatically uploaded to the Event Streams schema registry, and named with the subject ID (which is based on the subject name strategy in use) and a random suffix. 

Auto-registration is enabled by default in the Confluent Platform schema registry client library. To disable it, set the auto.register.schemas property to false.

{% include note.html content="To auto-register schemas in the Event Streams schema registry, you need an API key that has operator role permissions (or higher) and permission to create schemas. You can generate API keys by using the ES UI or CLI." %}

1. Log in to your Event Streams UI as an administrator from a supported web browser (see how to determine the login URL for your Event Streams UI).

1. Click Connect to this cluster on the right.

1. Click Generate API key.

1. Enter a name for your application, and select Produce, consume, create topics and schemas.

1. Click Next.

1. Enter your topic name or set All topics to On.

1. Click Next.

1. Enter the consumer group name or set All consumer groups to On.

1. Click Generate API key to generate an API key.

1. Click Copy API key.


##### Migrating schemas manually

To manually migrate the schemas, download the schema definitions from the Confluent Platform schema registry, and add them to the Event Streams Schema Registry. When manually adding schemas to the Event Streams Schema Registry, the provided schema name must match the subject ID used by the Confluent Platform schema registry subject name strategy.

If you are using the default TopicNameStrategy, the schema name must be <TOPIC_NAME>-<'value'|'key'>

If you are using the RecordNameStrategy, the schema name must be <SCHEMA_DEFINITION_NAMESPACE>.<SCHEMA_DEFINITION_NAME>

For example, if you are using the default TopicNameStrategy as your subject name strategy, and you are serializing your data into the message value and producing to the MyTopic topic, then the schema name you must provide when adding the schema in the UI must be MyTopic-value

For example, if you are using the RecordNameStrategy as your subject name strategy, and the schema definition file begins with the following, then the schema name you must provide when adding the schema in the UI must be org.example.Book:

```
{
    "type": "record",
    "name": "Book",
    "namespace": "org.example",
    "fields": [
...
```

If you are using the CLI, run the following command when adding the schema:

```
cloudctl es schema-add --create --name org.example.Book --version 1.0.0 --file /path/to/Book.avsc
```

#### Migrating a Kafka producer application

To migrate a Kafka producer application that uses the Confluent Platform schema registry, secure the connection from your application to Event Streams, and add additional properties to enable the Confluent Platform schema registry client library to interact with the Event Streams schema registry.

1. Configure your producer application to secure the connection between the producer and Event Streams.

1. Retrieve the full URL for the Event Streams API endpoint, including the host name and port number by using the cloudctl es init command.

1. Ensure you add the following schema properties to your Kafka producers:

	| Property Name | Property Value |
	|:----------:|:-------:|
	| schema.registry.url | https://<host name>:<API port> |
	| basic.auth.credentials.source | SASL_INHERIT | 
	
	You can also use the following code snippet for Java applications:
	
	```
	props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, "https://<host name>:<API port>");
props.put(AbstractKafkaAvroSerDeConfig.BASIC_AUTH_CREDENTIALS_SOURCE, "SASL_INHERIT");
	```
	
1. Set the Java SSL truststore JVM properties to allow the Confluent Platform schema registry client library to make HTTPS calls to the Event Streams schema registry. For example:

	```
	export KAFKA_OPTS="-Djavax.net.ssl.trustStore=/path/to/es-cert.jks \ 
      -Djavax.net.ssl.trustStorePassword=password"
   ```
   
#### Migrating a Kafka consumer application

To migrate a Kafka consumer application that uses the Confluent Platform schema registry, secure the connection from your application to Event Streams, and add additional properties to enable the Confluent Platform schema registry client library to interact with the Event Streams schema registry.

1. Configure your consumer application to secure the connection between the consumer and Event Streams.

1. Retrieve the full URL for the Event Streams API endpoint, including the host name and port number by using the cloudctl es init command.

1. Ensure you add the following schema properties to your Kafka producers:

	| Property Name | Property Value |
	|:----------:|:-------:|
	| schema.registry.url | https://<host name>:<API port> |
	| basic.auth.credentials.source | SASL_INHERIT | 

	You can also use the following code snippet for Java applications:
	
	```
	props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, "https://<host name>:<API port>");
props.put(AbstractKafkaAvroSerDeConfig.BASIC_AUTH_CREDENTIALS_SOURCE, "SASL_INHERIT");
    ```
1. Set the Java SSL truststore JVM properties to allow the Confluent Platform schema registry client library to make HTTPS calls to the Event Streams schema registry. For example:    

	```
	export KAFKA_OPTS="-Djavax.net.ssl.trustStore=/path/to/es-cert.jks \ 
       -Djavax.net.ssl.trustStorePassword=password"
	```

### Using schemas with the REST producer API

You can use schemas when producing messages with the Event Streams REST producer API. You simply add the following parameters to the API call:

* schemaname: The name of the schema you want to use when producing messages.
* schemaversion: The schema version you want to use when producing messages.

For example, to use cURL to produce messages to a topic with the producer API, and specify the schema to be used, run the curl command as follows:

```
curl "https://192.0.2.171:30342/topics/<topicname>/records?schemaname=<schema-name>&schemaversion=<schema-version-name>" -d '<avro_encoded_message>' -H "Content-Type: application/json" -H "Authorization: Bearer <apikey>" --cacert es-cert.pem
``` 

By adding these parameters to the API call, a lookup is done on the specified schema and its version to check if it is valid. If valid, the correct message headers are set for the produced message.

{% include important.html content="When using the producer API, the lookup does not validate the data in the request to see if it matches the schema. Ensure the message conforms to the schema, and that it has been encoded in the Apache Avro binary or JSON encoding format. If the message does not conform and is not encoded with either of those formats, consumers will not be able to deserialize the data." %}I

If the message has been encoded in the Apache Avro binary format, ensure the HTTP Content-Type header is set to application/octet-stream.

If the message has been encoded in the Apache Avro JSON format, ensure the HTTP Content-Type header is set to application/json.

### Using the schema API

Event Streams provides a Java library to enable Kafka applications to serialize and deserialize messages using schemas stored in your Event Streams schema registry. Using the schema registry serdes library API, schema versions are automatically downloaded from your schema registry, checked to see if they are in a disabled or deprecated state, and cached. The schemas are used to serialize messages produced to Kafka and deserialize messages consumed from Kafka.

Schemas downloaded by the schema registry serdes library API are cached in memory with a 10 minute expiration period. This means that if a schema is deprecated or disabled, it might take 10 minutes before consuming or producing applications will see the change. To change the expiration period, set the SchemaRegistryConfig.PROPERTY_SCHEMA_CACHE_REFRESH_RATE configuration property to a new milliseconds value.

